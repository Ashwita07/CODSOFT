{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwjsucA6JXBAcHX5uHRjsN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" TASK 1: Movie_Genre_Classification"],"metadata":{"id":"xP0ji197jbPZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHulHsL3bXvS","executionInfo":{"status":"ok","timestamp":1766463971474,"user_tz":-330,"elapsed":7198,"user":{"displayName":"Ashwitha","userId":"15722594665015489814"}},"outputId":"dd65d868-47fa-450d-8439-6920a584fa99"},"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file extracted successfully\n","Training file found at: /content/Genre Classification Dataset/train_data.txt\n","Dataset shape: (54214, 4)\n","   id                             title     genre  \\\n","0   1      Oscar et la dame rose (2009)     drama   \n","1   2                      Cupid (1997)  thriller   \n","2   3  Young, Wild and Wonderful (1980)     adult   \n","3   4             The Secret Sin (1915)     drama   \n","4   5            The Unrecovered (2007)     drama   \n","\n","                                                plot  \n","0  Listening in to a conversation between his doc...  \n","1  A brother and sister with a past incestuous re...  \n","2  As the bus empties the students for their fiel...  \n","3  To help their unemployed father make ends meet...  \n","4  The film's title refers not only to the un-rec...  \n","\n","Model Accuracy: 0.5225491100249009\n","\n","Sample Predictions:\n","Plot: A brave police officer fights criminals to save the city\n","Predicted Genre: drama\n","--------------------------------------------------\n","Plot: Two college students fall in love but face family pressure\n","Predicted Genre: drama\n","--------------------------------------------------\n","Plot: A family moves into a haunted house with supernatural events\n","Predicted Genre: horror\n","--------------------------------------------------\n","Plot: A detective investigates a mysterious murder case\n","Predicted Genre: thriller\n","--------------------------------------------------\n"]}],"source":["# =========================================\n","# Task 1: Movie Genre Classification\n","# =========================================\n","\n","import zipfile\n","import os\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Extract ZIP\n","zip_path = \"/content/movies.csv.zip\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"/content\")\n","\n","print(\"ZIP file extracted successfully\")\n","\n","# 2. Find train_data.txt recursively\n","train_file = None\n","for root, dirs, files in os.walk(\"/content\"):\n","    for file in files:\n","        if file == \"train_data.txt\":\n","            train_file = os.path.join(root, file)\n","            break\n","\n","if train_file is None:\n","    raise FileNotFoundError(\"train_data.txt not found\")\n","\n","print(\"Training file found at:\", train_file)\n","\n","# 3. Load training data (TXT format)\n","data = pd.read_csv(\n","    train_file,\n","    sep=\" ::: \",\n","    engine=\"python\",\n","    names=[\"id\", \"title\", \"genre\", \"plot\"]\n",")\n","\n","print(\"Dataset shape:\", data.shape)\n","print(data.head())\n","\n","# 4. Separate input and output\n","X = data[\"plot\"]\n","y = data[\"genre\"]\n","\n","# 5. TF-IDF Vectorization\n","tfidf = TfidfVectorizer(\n","    stop_words=\"english\",\n","    max_features=5000\n",")\n","\n","X_tfidf = tfidf.fit_transform(X.astype(str))\n","\n","# 6. Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_tfidf, y,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","# 7. Train model\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","\n","# 8. Accuracy\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"\\nModel Accuracy:\", accuracy)\n","\n","# 9. Sample predictions\n","test_movies = [\n","    \"A brave police officer fights criminals to save the city\",\n","    \"Two college students fall in love but face family pressure\",\n","    \"A family moves into a haunted house with supernatural events\",\n","    \"A detective investigates a mysterious murder case\"\n","]\n","\n","test_tfidf = tfidf.transform(test_movies)\n","predictions = model.predict(test_tfidf)\n","\n","print(\"\\nSample Predictions:\")\n","for movie, genre in zip(test_movies, predictions):\n","    print(\"Plot:\", movie)\n","    print(\"Predicted Genre:\", genre)\n","    print(\"-\" * 50)\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"mEh7GYPhdiLD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TASK:2 CUSTOMER_CHURN_PREDICTION\n"],"metadata":{"id":"ujPvLz-RjRTq"}},{"cell_type":"code","source":["# ===============================\n","# CUSTOMER CHURN PREDICTION\n","# ===============================\n","\n","import zipfile\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# ---------- 1. UNZIP DATASET ----------\n","zip_path = \"/content/archive (7).zip\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/data\")\n","\n","# ---------- 2. LOAD CSV FILE ----------\n","for file in os.listdir(\"/content/data\"):\n","    if file.endswith(\".csv\"):\n","        df = pd.read_csv(\"/content/data/\" + file)\n","        break\n","\n","# ---------- 3. DROP USELESS COLUMNS ----------\n","df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n","\n","# ---------- 4. SEPARATE FEATURES & TARGET ----------\n","X = df.drop('Exited', axis=1)\n","y = df['Exited']\n","\n","# ---------- 5. ENCODE CATEGORICAL DATA ----------\n","le = LabelEncoder()\n","X['Geography'] = le.fit_transform(X['Geography'])\n","X['Gender'] = le.fit_transform(X['Gender'])\n","\n","# ---------- 6. TRAIN TEST SPLIT ----------\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# ---------- 7. SCALE DATA (FOR LOGISTIC REGRESSION) ----------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# ---------- 8. LOGISTIC REGRESSION ----------\n","lr = LogisticRegression(max_iter=5000)\n","lr.fit(X_train_scaled, y_train)\n","y_pred_lr = lr.predict(X_test_scaled)\n","\n","print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n","\n","# ---------- 9. RANDOM FOREST ----------\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","y_pred_rf = rf.predict(X_test)\n","\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"],"metadata":{"id":"rh_5AZeGdh04","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766465475596,"user_tz":-330,"elapsed":1946,"user":{"displayName":"Ashwitha","userId":"15722594665015489814"}},"outputId":"e593a6ec-ad74-4bf0-f729-85a7eeb3d68a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy: 0.8155\n","Random Forest Accuracy: 0.8645\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lHgNWGatj1Tb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" TASK:3 SPAM_SMS_DETECTION\n","\n","\n"],"metadata":{"id":"kt-jbswpnCms"}},{"cell_type":"code","source":["# ================================\n","# SPAM SMS DETECTION\n","# ================================\n","\n","# 1. Import libraries\n","import pandas as pd\n","import zipfile\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# 2. Unzip dataset\n","zip_path = \"/content/archive (8).zip\"\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"/content\")\n","\n","print(\"ZIP file extracted\")\n","\n","# 3. Load dataset (common file name: spam.csv)\n","data = pd.read_csv(\"/content/spam.csv\", encoding=\"latin-1\")\n","\n","data = data[['v1', 'v2']]\n","data.columns = ['label', 'message']\n","\n","# 4. Convert labels to numbers\n","data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n","\n","# 5. Split data\n","X = data['message']\n","y = data['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# 6. TF-IDF vectorization\n","tfidf = TfidfVectorizer(stop_words='english')\n","\n","X_train_tfidf = tfidf.fit_transform(X_train)\n","X_test_tfidf = tfidf.transform(X_test)\n","\n","# 7. Train Naive Bayes model\n","model = MultinomialNB()\n","model.fit(X_train_tfidf, y_train)\n","\n","# 8. Evaluate model\n","y_pred = model.predict(X_test_tfidf)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Model Accuracy:\", accuracy)\n","\n","# 9. Prediction function\n","def check_sms(text):\n","    text_tfidf = tfidf.transform([text])\n","    prediction = model.predict(text_tfidf)\n","\n","    if prediction[0] == 1:\n","        print(\"SPAM MESSAGE\")\n","    else:\n","        print(\"HAM (NOT SPAM)\")\n","\n","# 10. Test messages\n","test_messages = [\n","    \"Congratulations! You won a free prize\",\n","    \"I will call you later\",\n","    \"Urgent! Claim your reward now\",\n","    \"Are you coming to college today?\",\n","    \"Limited offer just for you\",\n","    \"Meeting is postponed to tomorrow\"\n","]\n","\n","for msg in test_messages:\n","    print(\"Message:\", msg)\n","    check_sms(msg)\n","    print(\"-\" * 40)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JCJEnnKUj10K","executionInfo":{"status":"ok","timestamp":1766465869504,"user_tz":-330,"elapsed":147,"user":{"displayName":"Ashwitha","userId":"15722594665015489814"}},"outputId":"0d301f72-213a-47e1-c226-c641ba9ca2b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file extracted\n","Model Accuracy: 0.9668161434977578\n","Message: Congratulations! You won a free prize\n","SPAM MESSAGE\n","----------------------------------------\n","Message: I will call you later\n","HAM (NOT SPAM)\n","----------------------------------------\n","Message: Urgent! Claim your reward now\n","SPAM MESSAGE\n","----------------------------------------\n","Message: Are you coming to college today?\n","HAM (NOT SPAM)\n","----------------------------------------\n","Message: Limited offer just for you\n","HAM (NOT SPAM)\n","----------------------------------------\n","Message: Meeting is postponed to tomorrow\n","HAM (NOT SPAM)\n","----------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jNGTB-d6l43s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" TASK:4 Handwritten_Text_Generatio"],"metadata":{"id":"cgwqT82kmyk_"}},{"cell_type":"code","source":["# Handwritten Text Generation using Character-Level RNN\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","# --------------------------------------------------\n","# Create handwritten-style text dataset\n","# --------------------------------------------------\n","text = \"\"\"\n","hello my name is Ashwitha\n","this project demonstrates handwritten text generation\n","recurrent neural networks learn character patterns\n","lstm models are useful for sequence data\n","machine learning internship task\n","\"\"\"\n","\n","with open(\"handwritten_text.txt\", \"w\") as f:\n","    f.write(text)\n","\n","with open(\"handwritten_text.txt\", \"r\") as f:\n","    text = f.read().lower()\n","\n","# --------------------------------------------------\n","#  Character encoding\n","# --------------------------------------------------\n","chars = sorted(list(set(text)))\n","char_to_idx = {c: i for i, c in enumerate(chars)}\n","idx_to_char = {i: c for i, c in enumerate(chars)}\n","\n","# --------------------------------------------------\n","# Create training sequences\n","# --------------------------------------------------\n","seq_len = 20\n","X = []\n","y = []\n","\n","for i in range(len(text) - seq_len):\n","    X.append([char_to_idx[c] for c in text[i:i + seq_len]])\n","    y.append(char_to_idx[text[i + seq_len]])\n","\n","X = np.array(X) / len(chars)\n","y = tf.keras.utils.to_categorical(y, num_classes=len(chars))\n","\n","# --------------------------------------------------\n","# Build RNN model\n","# --------------------------------------------------\n","model = Sequential()\n","model.add(LSTM(128, input_shape=(seq_len, 1)))\n","model.add(Dense(len(chars), activation=\"softmax\"))\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n","\n","# --------------------------------------------------\n","# Train the model\n","# --------------------------------------------------\n","model.fit(\n","    X.reshape(X.shape[0], seq_len, 1),\n","    y,\n","    epochs=20,\n","    batch_size=32\n",")\n","\n","# --------------------------------------------------\n","# Generate handwritten-style text\n","# --------------------------------------------------\n","def generate_text(start_text, length=200):\n","    result = start_text.lower()\n","\n","    while len(result) < seq_len:\n","        result = \" \" + result\n","\n","    for _ in range(length):\n","        seq = [char_to_idx[c] for c in result[-seq_len:]]\n","        seq = np.array(seq) / len(chars)\n","        seq = seq.reshape(1, seq_len, 1)\n","\n","        pred = model.predict(seq, verbose=0)\n","        result += idx_to_char[np.argmax(pred)]\n","\n","    return result\n","\n","print(\"\\nGenerated Text:\\n\")\n","print(generate_text(\"this project \"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbdnLwJLl4sh","executionInfo":{"status":"ok","timestamp":1766466345815,"user_tz":-330,"elapsed":24955,"user":{"displayName":"Ashwitha","userId":"15722594665015489814"}},"outputId":"b466ac26-5a80-4060-d7d9-c41ee81dd80a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 3.2091\n","Epoch 2/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.1466\n","Epoch 3/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.0262\n","Epoch 4/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.9219\n","Epoch 5/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.9014\n","Epoch 6/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.8558\n","Epoch 7/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.8342\n","Epoch 8/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.8317\n","Epoch 9/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.8652\n","Epoch 10/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2.7981\n","Epoch 11/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.8833\n","Epoch 12/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 2.8624\n","Epoch 13/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.8082\n","Epoch 14/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.7809\n","Epoch 15/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.8668\n","Epoch 16/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.8191\n","Epoch 17/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.8349\n","Epoch 18/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.8293\n","Epoch 19/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.8838\n","Epoch 20/20\n","\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.8699\n","\n","Generated Text:\n","\n","       this project eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n"]}]}]}